\documentclass[a4paper,12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks = true, citecolor=red, urlcolor=blue]{hyperref}

\parindent=2pt
\parskip=1ex plus 0.5ex minus 0.2ex

\begin{document}

\title{Repeated games with mistakes}
\author{Nikolas M. Skoufis \\ Supervisor: Julian Garcia}
\date{October 20th, 2015}

\maketitle

\begin{abstract}

This project studies efficient methods to compute the expected payoff of an iterated prisoner's dilemma between two opponents.
In order to efficiently compute these payoffs, the mathematics underpinning the iterated prisoners dilemma and the computation of expected payoff was analyzed with a view to improving in speed of computation over existing methods.
A Monte-Carlo method of computing expected payoff was used as a benchmark against which two fast methods for computing payoff were compared.
The results of these fast methods were then used to generate three-population simplex plots of evolutionary dynamics as a further benchmark.

\end{abstract}

\section*{Introduction}

\subsection*{Prisoner's Dilemma}

The iterated prisoner's dilemma is an archetypal problem in game theory, particularly in the study of the evolution of cooperation.
The iterated prisoner's dilemma involves two prisoners, each of which has two moves available to them.
The prisoner's are told of the two options available to them, and they are told that they have 24 hours to make a decision or they will be punished.
Each player can either inform on the other player (known as \textit{defection}) or stay silent (known as \textit{cooperation}).
If both prisoners defect, will both spend 10 years in prison.
If both prisoners cooperate, they will each spend only 5 years in prison; a better outcome for both.
However if one prisoner cooperates and the other defects, the prisoner who defected goes free, and the prisoner that cooperated will spend 20 years in prison; longer than any other option.

Given these options and outcomes, we can see that a natural dilemma arises.
If a player cooperates, they may receive a better outcome than if they had defected, depending on the other player's action.
However if they defect, they have the opportunity for best possible outcome (going free), but they also risk punishment if the other player defects.
This problem is the prisoner's dilemma.

An important concept in game theory is that of the Nash equilibrium.
The Nash equilibrium is a set of strategies where each player cannot improve their own outcome by changing their strategy while the opponent's strategy remains the same.
This means that in effect, a Nash equilibrium is the optimal result for all players.
It is a well known result in game theory that the only Nash equilibrium in the prisoner's dilemma is for both players to defect.
However this result is only valid for the single-shot prisoner's dilemma ie. the prisoner's dilemma where the players only play the game once.
If players player multiple times, more complex strategies can emerge and there emerges an incentive for mutual cooperation~\cite{trivers}.
This problem is known as the `iterated prisoner's dilemma'.

One issue with the iterated prisoner's dilemma is that if players know ahead of time how many rounds of the prisoner's dilemma they will play, the cooperation breaks down.
Since there are no repercussions for defecting in the final round (because there are no more rounds in which the opponent can retaliate), it is always best to defect.
However if it is optimal for both players to defect in the final round, then logically there are no repercussions for defecting in the second-to-last round either.
By this logic, the optimal strategy is simply to always defect.
In order to overcome this predictable and uninteresting pattern, we introduce the idea that the game length is determined by a repeated random event.
After playing the first and subsequent rounds, the game continues for another round with probability $0 < \delta < 1$.
This means that there is a always a chance that there will be another round of the game, and thus there is always the possibility that defection will carry consequences.

\subsection*{Expected Payoff}

The different moves and their respective outcomes of the prisoner's dilemma are generally abbreviated, and we will use these abbreviations from here onwards.
Cooperation and defection are usually abbreviated to C and D respectively.
The payoff for mutual cooperation is abbreviated to R for reward.
The payoff for mutual defection is abbreviated to P for punishment.
If one player defects and the other cooperates, the defector is said to receive the T (for temptation) payoff and the cooperator receives the S (for sucker) payoff.

When playing the iterated prisoner's dilemma, many different strategies are possible.
The most basic of strategies is to unconditionally perform the same action: unconditionally defect or unconditionally cooperate.
These strategies are known as Always Defect (AllD) and Always Cooperate (AllC) respectively.
More complicated strategies include:

\begin{itemize}
    \item Tit For Tat (TFT): If the opponent cooperate last round, you cooperate. If the opponent defected last round, you defect. In the first round, the player cooperates.
    \item Win Stay, Lose Shift (WSLS): If the outcome is a win for the player (R payoff or T payoff) then the previous move is repeated. If the outcome is a lose for the player (P payoff or S payoff) then the opposite move is chosen in the next round.
    \item Grim: Cooperates until the opponent defects, and then unconditionally defects from then on.
\end{itemize}

It is useful to be able to compare the effectiveness of each of these strategies when paired against another strategy.
For simple cases such as AllC against AllD, AllD is trivially the superior strategy.
However in more complicated cases, it becomes necessary to use a metric to evaluate the performance of a strategy.
The most commonly used metric is the expected payoff.

The expected payoff is defined as the expected value of the payoff of one strategy when pitted against another strategy.
For deterministic strategies, this can easily be computed with an infinite sum \cite{garciaandtraulsen}:

\begin{equation}
    \sum_{i=0}^{\infty} \delta^i \pi_i
\end{equation}

where $\delta$ is the continuation probability and $\pi_i$ is the payoff of the strategy in round $i$.

This sum holds for any pair of deterministic strategies.
This sum is straight-forward to compute programatically, however when we move to non-deterministic strategies it quickly becomes difficult.
Any strategy that employs non-deterministic play styles (eg. defect 75\% of the time, cooperate the other 25\%) introduces additional options in the outcome space which must be accounted for.
We must now multiply the continuation probability by the probability of that particular outcome, and we have to continue this `branch' of the outcome tree forever.
For more complex strategies, the space of outcomes quickly becomes very large, and consequently, the expected payoff becomes very difficult to compute.

\subsection*{Mistakes and fault tolerant strategies}

Another possible complication of the outcome space is the addition of mistakes.
Mistakes occur when a player intends to play a particular move, but instead they play a different move.
In the context of the iterated prisoner's dilemma, this occurs when a player intends to cooperate, but instead they defect (or vice-versa).
Since mistakes are probabilistic and can occur at any round, they serve to worsen the problem of exploding outcome space and associated difficulty in calculating expected payoffs.
This project's primary goal was to determine a method of computing the expected payoff of an iterated prisoner's dilemma with mistakes in an efficient manner.

In the presence of mistakes, some strategies fare better than others.
In particular, it is a well known result \cite{axelrod} that the simple TFT strategy outperforms all other strategies in the absence of mistakes.
However in the presence of mistakes, the TFT strategy displays poor `fault tolerance' because it loses the `robustness' property.
For a rigorous definition of fault tolerance and robustness, refer to \cite{pelc} and \cite{pelcpelc}.
Briefly, robustness is a measure of how a strategy performs against another strategy in a population consisting only of those two strategies.
Informally, a robust strategy is said to be sometimes better and never worse than other strategies.
A fault tolerant strategy is a strategy that is able to retain this robust quality, even in the face of mistakes.

Since TFT is not particularly fault tolerant, modern game theorists have attempted to study strategies that are able to deal with faults while still remaining competitive.
And in order to evaluate these strategies, it becomes necessary to efficiently and easily calculate their expected payoffs against other strategies.

\section*{Software tools and techniques}

In order to study the expected payoff, it was necessary to first design a software framework that allows for the construction, simulation and computation of scenarios in game theory.
The code used in this project is open sourced under the GNU GPL v3 license and is available at https://github.com/computationalevolutionarydynamics/repeatedmistakes.

Python was chosen as the language for development of this framework, due to both the author's familiarity with the language coupled with high quality libraries for numerical calculations, visualization and testing.
In particular, this project made use of the Hypothesis property-based testing library \cite{hypothesis} and the Nose testing framework \cite{nose}.
I also used the Travis CI continuous integration service and the Coveralls test coverage service.

\subsection*{Hypothesis and Nose}
The Hypothesis library is inspired by the Haskell library `Quickcheck', and is a property-based form of testing.
The chief difference is that instead of defining in terms of specific inputs and outputs, you define tests in terms of classes of input and their expected output.
Hypothesis uses in intelligent search strategy to attempt to produce an input which falsifies the property.
Hypothesis then reduces this input to the minimum falsifying example and returns this.

As an example, here is how we might test a \texttt{reverse} method with conventional testing and with Hypothesis:

\begin{itemize}
    \item Conventional: when called with the input \texttt{foobar}, the method should return \texttt{raboof}.
    \item Hypothesis: when called with any string, the method should return the reverse of that string.
\end{itemize}

Clearly the Hypothesis version is much more powerful and expressive.
For example, if the \texttt{reverse} method didn't correctly handle empty strings, then Hypothesis would catch this and return it to the user.

Hypothesis proved to be an invaluable tool in this project because it allowed me to define the general properties that should be obeyed for any strategy and any history, and then evaluate my implementations of the strategies against these properties.

One issue I ran into with using Hypothesis was running tests can be slow when compared to standard unit testing.
For example, running a test to ensure that strategies always return the correct length of history over 8 different strategies can take on the order of seconds.
Although this is not a huge concern, this did cause some issues with Travis as discussed below.

Another issue with Hypothesis is that occasionally it would produce an error because it was unable to produce enough test data satisfied the requirements given.
This is most likely due to inexperience with Hypothesis, and could likely be remedied by optimizing the functions that construct test data.

Finally, since Hypothesis's testing is fundamentally probabilistic in nature, occasionally tests would fail even though the code that changed did not affect them.
This occurs because Hypothesis uses some randomized test data to find edge cases, and sometimes that test data doesn't hit upon a falsifying example.
This can be remedied by increasing the number of examples to test for each property, however there is always the possibility that a falsifying example will be skipped over.

In addition to Hypothesis, I made use of the Nose testing framework.
Nose aims to be a replacement for Python's default \texttt{unittest} library, with an emphasis on simpler and more readable test code.
A key reason behind my use of Nose is that Nose allows for what it calls 'test generators'.
Test generators allow for generation of multiple tests over a variety of input data.
This feature was also useful when evaluating the correctness of the strategies that were implemented.

\subsection*{Travis and Coveralls}

In order to easily keep track of the status of the unit tests, I used the Travis CI continuous integration service.
Travis CI can be configured to automatically build your project, and then to run any tests you have implemented.
Travis builds and runs the tests upon every Github commit, so it is a valuable tool in ensuring that new code does not cause tests to fail.

As mentioned in the previous section, some issues occurred when using Travis with Hypothesis.
Since some tests took a very long time to execute, an additional option had to be set in Travis to allow for long periods of time without output.
Even with this setting, builds sometimes failed because Travis has an upper limit on how long it will wait before timing out.

To assess the test coverage of the tests I had written, I used the Coveralls service.
This service integrates with Travis to automatically generate test coverage reports.

Both of these services offer badges (dynamically updated images) that can be displayed on Github so that it's easy to see at a glance whether your build is passing and how much of the code base is covered by your tests.

\section*{Software framework}

To this end I implemented a \texttt{Strategy} class which implements basic functionality of a strategy such as tracking the player's history and computing the next move for the player.
This \texttt{Strategy} class is never instantiated directly, but is instead subclassed by concrete implementations of strategies eg. \texttt{AllC}, \texttt{AllD}, \texttt{TitForTat} etc.
This implementation should allow for any strategy to be implemented (including non-deterministic strategies) since the method that computes the next move for the strategy knows about both the player's own history as well as being passed the opponent's history.


\begin{thebibliography}{9}

    \bibitem{axelrod}
        Robert Axelrod,
        \emph{Effective choice in the Prisoner's Dilemma},
        The Journal of Conflict Resolution,
        Vol. 24,
        No. 1,
        1980.

    \bibitem{pelcpelc}
        Andrzej Pelc, Krzysztof J. Pelc,
        \emph{Same Game, New Tricks: What Makes a Good Strategy in the Prisoner's Dilemma?},
        The Journal of Conflict Resolution,
        Vol. 35,
        No 5,
        2009.

    \bibitem{pelc}
        Andrzej Pelc,
        \emph{Fault-tolerant strategies in the Iterated Prisoner's Dilemma},
        Information Processing Letters,
        Vol. 110,
        No. 10,
        2010.

    \bibitem{garciaandtraulsen}
        Julian Garcia, Arne Traulsen,
        \emph{The Structure of Mutations and the Evolution of Cooperation},
        PLoS ONE,
        Vol. 7,
        No. 4,
        2012.

    \bibitem{trivers}
        Robert Trivers,
        \emph{The Evolution of Reciprocal Altruism},
        The Quarterly Review of Biology,
        Vol. 46,
        No. 1,
        1971.

    \bibitem{hypothesis}
        David R. MacIver,
        \emph{Hypothesis},
        https://github.com/DRMacIver/hypothesis,
        2015.

    \bibitem{nose}
        \emph{Nose},
        https://nose.readthedocs.org/en/latest/index.html,
        Version 1.3.7.


    \bibitem{ross}
        Sheldon M. Ross,
        \emph{Simulation},
        Fourth edition,
        Elsevier Academic Press,
        2006.

\end{thebibliography}

\end{document}
